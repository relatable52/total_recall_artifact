# *Total Recall? How Good are Static Call Graphs Really?* - Companion Artifact

This is the artifact for our paper *Total Recall? How Good are Static Call Graphs Really?*, which was accepted at ISSTA 2024. The paper proposes a novel methodology for obtaining dynamic callgraphs to judge the precision and recall of callgraphs generated by static analysis frameworks. To generate meaningful inputs for dynamic callgraph runs, we rely on existing input corpora, as well as fuzzing.

This artifact holds all our implementations, evaluation scripts and data necessary to reproduce our findings. 

## Getting started
Make sure you have local installations of Docker and Java (1.8 or higher). Install the `just` [command runner](https://github.com/casey/just) according to your operating system (e.g. `choco install just` for Windows and Chocolatey).

To validate your setup you can run `just --list` in the top level directory of this artifact. The output should show a list of thirteen recipes available.

To reproduce our findings on a smaller scale - and for a single project - run the following commands in order:

* `cd ./projects/axion/` - Change into a specific project directory to run commands only that project (here: `axion`)
* `just clean` - *Optionally* remove all intermediate results that we shipped for this project - you can also keep those, then subsequent commands will build ontop of them
* `just FUZZING_TIME="600" coverage_fuzzing_seed` - Run fuzzing with our base coporus for ten minutes to generate additional inputs for the project.
   * If you are interested in the coverage that the fuzzing run achieved, look into `./target/fuzzing_seed/coverage` for a full HTML report
   * You can execute this command repeatedly to generate further inputs and increase coverage
* `just dynamic_callgraph_fuzzing_seed` - Record a dynamic callgraph with all inputs generated via fuzzing so far
* `just static_callgraphs` - Compute all static callgraphs for this project
   * Note that this might require a lot of RAM (126GB or higher) and time (multiple hours).
   * If you do not have those resources available, you can also copy the static callgraphs we computed for all our projects from this artifact's supplementary material.
* `just compare_callgraphs` - Compute precision and recall measures for all static callgraphs using the dynamic recording as a baseline.
   * Results can be found in `./target/comparecg`
* **[Optionally]** Run `cd ../..` and `just build_plots` to regenerate all plots in `plots/results` using the new data you just gathered for the `axion` project

This completes the entire pipeline that we proposed in our paper for a single project. The next section holds more detailed information on how to use the tools and data contained in this artifact.

## Detailed Description

This artifact holds a pipeline that captures a *dynamic callgraph* for a JVM program and a given set of inputs (input corpus). This dynamic callgraph can then be used as a *baseline* to compute precision and recall of a defined set of static callgraphs.

To achieve a good quality of the dynamic baseline, the pipeline provides different techniques for creating a suitable input corpus. These are:

* **Base Seed Corpus**: Pre-existing input corpora found online, without any modification
* **Seed Corpus**: Manual additions to the Base Seed Corpus derived from inspecting the coverage values.
* **Fuzzing**: A coverage-guided fuzzer ([Jazzer](https://github.com/CodeIntelligenceTesting/jazzer)) generates program inputs from scratch
* **Fuzzing Seed**: Jazzer generates new inputs using the Seed Corpus as a starting point. This is the combination of all aforementioned techniques, which we found to be best suited for good quality dynamic callgraphs.

The pipeline evaluates the following fixed set of static callgraphs: 

* **OPAL**: CHA, RTA, 0-CFA
* **WALA**: CHA, RTA, 0-CFA
* **Soot**: CHA
* **Doop**: 0-CFA

Numerical values for precision and recall are computed for every static callgraph and every project. We further include scripts that visualize those values for our set of four programs.

### Prerequisites 
In order to use the tools provided in this packages, you will need local installations of:
* Docker
* The `just` [command runner](https://github.com/casey/just)
* Java (1.8 or higher)

We rely on the `just` command runner to execute commands. It supports execution of different *Recipes* which define (parameterized) sets of commands that are to be executed.

Some of the computations executed here are very resource-intensive, we recommend at least 64GB of RAM to record dynamic callgraphs and 128GB to compute static callgraphs. We performed our experiments on servers with 120GB and 400GB of RAM, respectively.

### Overview
We conducted our experiments on a benchmark of four projects (`axion`, `batik`, `jasml`, `xerces`), which are contained in the `projects` directory. `/projects/Justfile` defines a number of recipes that can be executed for a single such project, while `/Justfile` contains recipes that work on the entire benchmark. 
```
/
|- <...>
|- projects
| |-- axion
| | |--- .env
| | ╹--- <...>
| |-- batik
| | |--- .env
| | ╹--- <...>
| |-- jasml
| | |--- .env
| | ╹--- <...>
| |-- xerces
| | |--- .env
| | ╹--- <...>
| ╹-- Justfile
╹- Justfile
```

### Run Recipes for a Single Project
To execute recipes for a single project of the benchmark, you first need to change into the respective project directory. For example, run `cd ./projects/batik` in the root directory to be able to run recipes for the `batik` project. Each project holds a `.env` file that configures project-specific variables and settings. To load those settings, simply run the following for any given recipe:

```
just --dotenv-path .env <recipe_name>
```

For example, you can run `just --dotenv-path .env clean` to clean all data for the current project. The following recipes are available for individual projects:

| Recipe       | Requires | Description | Outputs To |
|--------------|:-----:|-----------|:-----:|
| `build_project` | Nothing | Compiles the current project including all dependencies. | `./target/repo` | 
| `coverage_base_seed`      |  Nothing | Calculates the coverage achieved when using just a publicly available corpus as inputs to the current program. | `./target/base_seed/coverage`| 
| `coverage_seed` | Nothing | Same as `coverage_base_seed`, but with our manual additions to the set of inputs. | `./target/seed/coverage` | 
| `coverage_fuzzing` | Nothing | Calculates the coverage achieved when using only fuzzer-generated input to the current program. Can be executed multiple times to iteratively increase the set of inputs. | `./target/fuzzing/coverage` | 
| `coverage_fuzzing_seed` | Nothing | Same as `coverage_fuzzing`, but the fuzzer uses the full existing input corpus as a starting point. |`./target/fuzzing_seed/coverage` | 
| `dynamic_callgraph_fuzzing_seed` | `coverage_fuzzing_seed` | Records a dynamic callgraph for the current program with all inputs generated via fuzzing (`coverage_fuzzing_seed`). | `./target/fuzzing_seed/dyncg` |
| `static_callgraphs` | Nothing | Builds static callgraphs for the current program for every framework and algorithm we included in our study (OPAL, Doop, WALA, Soot) | `./target/staticcg` | 
| `clean` | Nothing | Removes all data that has been computed for this project so far (inside the `target` directory). Note that this resets the input corpora generate via fuzzing for this project. | / |
| `compare_callgraphs` | `dynamic_callgraph_fuzzing_seed` && `static_callgraphs` | Compares the dynamically recorded callgraph for this project to all static callgraphs. Computes precision and recall values for every static callgraph compared to the dynamic one. Requires a dynamic callgraph to be recorded and all static callgraphs to be computed first. | `./target/comparecg`|  

There are parameters that can be configured to change the default behavior of recipes. Parameters can be set when invoking a recipe: `just <PARAM>=<VALUE> <recipe_name>`.
The most important parameters in our case are:
- `FUZZING_THREADS := "8"`: Number of threads the fuzzer is executed with. Applies to all Fuzzing-Related Recipes.
- `FUZZING_TIME := "3600"`: Number of seconds to run the fuzzer. Applies to all Fuzzing-Related Recipes.
- `DYNCG_TIMEOUT := "999m"`: Timeout for computing the dynamic call graphs. Applies to `dynamic_callgraph_fuzzing_seed`.
- `DYNCG_MEMORY := "400G"`: Memory for computing the dynamic call graphs. Applies to `dynamic_callgraph_fuzzing_seed`.
- `STATICCG_TIMEOUT := "90m"`: Timeout for computing the static call graphs. Applies to `static_callgraphs`.
- `STATICCG_MEMORY := "400G"`: Memory to computing the static call graphs. Applies to `static_callgraphs`.

**To reproduce our core experiments**, one would typically want to build a reasonably large input corpus using the fuzzer, then record the dynamic callgraph, compute all static callgraphs, and finally compute the precision and recall for all static callgraphs given the dynamic one. This would be done by executing the following commands (with 10 minutes of fuzzing):

```
just --dotenv-path .env FUZZING_TIME="600" coverage_fuzzing_seed
just --dotenv-path .env dynamic_callgraph_fuzzing_seed
just --dotenv-path .env static_callgraphs
just --dotenv-path .env compare_callgraphs
```

**NOTE:** Some recipes may take a very long time to execute. Especially `static_callgraphs` will require a lot of resources and may take many hours to complete. Recording a dynamic callgraph via `dynamic_callgraph_fuzzing_seed` may also take several hours, or even days, dependending on the resources available and the size of the input corpus.

### Run Recipes for the Entire Benchmark
To execute recipes for every project of the benchmark, you need to run commands in the root directory of this artifact. This does not involve `.env` files, they are loaded automatically for the respective projects. The following recipes are available and work exactly as specified for single projects (including their requirements):

* `coverage_seed`
* `coverage_fuzzing`
* `coverage_fuzzing_seed`
* `dynamic_callgraph_fuzzing_seed`
* `static_callgraphs`
* `compare_callgraphs`
* `clean`

Additionally, the following recipe is avilable only at top-level:

| Recipe       | Requires | Description | Outputs To |
|--------------|:-----:|-----------|:-----:|
| `build_plots` | `compare_callgraphs` | Builds the plots for precision and recall that we used for our evaluation. | `./plot/results/` | 

**To regenerate the plots we used in our paper entirely from scratch**, one has to execute the following command sequence inside the root directory (you can vary the `FUZZING_TIME` parameter to increase coverage of the input corpus):
```
just FUZZING_TIME="600" coverage_fuzzing_seed
just dynamic_callgraph_fuzzing_seed
just static_callgraphs
just compare_callgraphs
just build_plots
```
**NOTE**: Running this sequence of commands will take a very long time to execute. Dependending on the input corpus size (which correlates to the `FUZZING_TIME`), recording a dynamic callgraph for all projects may take multiple days.

### Intermediate Results Included
This artifact contains a number of intermediate results that are meant to enable you to validate our results and to execute most recipes without having to wait for long, resource-intensive computations. These results can be found in each project's `target` directory. We included:
* The results of `coverage_seed` for all projects. (`./projects/<project>/target/seed`)
* The results of `coverage_base_seed` for all projects. (`./projects/<project>/target/base_seed`)
* The input corpus and coverage generated by `coverage_fuzzing_seed` for all projects with two threads and two hour timeout for the fuzzer. (`./projects/<project>/target/fuzzing_seed`)
* The results of `compare_callgraphs` for all projects. This is the data we based our visualizations for the paper on. (`./projects/<project>/target/comparecg`)
* The results of `dynamic_callgraph_fuzzing_seed` for all projects. (`./projects/<project>/target/fuzzing_seed/dyncg`)
* The results of `build_plots`. These are the visualizations that we used for our paper. (`./plot/results`)

The results of `static_callgraphs` for all projects are available as an additional supplementary archive, as their compressed filesize is around 1GB. It contains a dedicated README that holds installation instructions.

### Validating our Claims
In our paper, we answered three research questions (RQs) based on the data contained in / obtained by this artifact. The following data was used for the corresponding RQs:
* **RQ1**: Quality of our dynamic callgraphs. Here we used the input set achieved by combining all input generation techniques to judge whether our approach yields sensible program coverages. The corresponding data can be found in `./projects/<project>/target/fuzzing_seed/coverage`.
* **RQ2**: Influence of individual input generation techniques. To judge which technique yields the most coverage, we compared the data available in `./projects/<project>/target/fuzzing_seed/coverage` (all techniques), `./projects/<project>/target/fuzzing/coverage`(fuzzing without base corpus), `./projects/<project>/target/base_seed/coverage` (pre-existing input corpus only) and `./projects/<project>/target/seed/coverage` (pre-existing input corpus with manual extensions).
* **RQ3**: Comparing static call graphs. We compared the precision- and recall-values for all benchmark programs and all static callgraphs by inspecting the plots generated via `just build_plots`, which can be found in `./plot/results`.

### Extending the benchmark
Conceptually, a couple of steps need to be taken to add new projects to the benchmark. One has to:

* Define a *Base Corpus* (set of inputs). The scripts `/projects/<project>/prepare_base_corpus.sh` are responsible for aggregating these base corpora.
* Optionally define manual extensions to the Base Corpus to form the *Seed Corpus*. The scripts `/projects/<project>/prepare_seed_corpus.sh` are responsible for aggregating these seed corpora and are executed after `prepare_base_corpus.sh` has been called.
* Find a suitable dictionary (`dict.txt`) to guide the fuzzing process. `/projects/<project>/prepare_dictionary.sh` copies those files to their destination for each project.
* Pick an entrypoint to your program and wrap it inside a main method. Examples can be found in `/projects/<project>/src/Entrypoint.java`.
* Create a [Fuzzer implementation](https://github.com/CodeIntelligenceTesting/jazzer/tree/main/examples/src/main/java/com/example) for your project. See `/projects/<project>/src/<project>Fuzzer.java`.
* Provide the sources for the new project. This is currently done via the XCorpus Docker image (`/java-corpora/xcorpus`) and limited to projects contained in the XCorpus benchmark.